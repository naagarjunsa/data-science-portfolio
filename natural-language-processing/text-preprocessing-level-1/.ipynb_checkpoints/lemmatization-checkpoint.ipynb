{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT PREPROCESSING - STEMMING\n",
    "\n",
    "**Stemming** is text preprocessing techinique where the tokens generated from the corpus are reduced to their base units. The base units need not be meaningful words. This makes it less complex and faster. Manual rule based way of cutting words down.\n",
    "* _go, going, gone --> go_\n",
    "\n",
    "\n",
    "**Overstemming** is when you stem too much of the token.\n",
    "* _universe, university, universities --> univers_\n",
    "\n",
    "**Understemming** is when you dont stem the token enough\n",
    "* datum , data -> dat ==> What about date?\n",
    "\n",
    "**StopWords** are words which do not add much meaning to the sentence.\n",
    "* a, an, the, is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#will be using NLTK to demonstrate stemming\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "paragraph = \"\"\"Paragraphs are the building blocks of papers. Many students define paragraphs \\\n",
    "in terms of length. A paragraph is a group of at least five sentences. Paragraph \\\n",
    "is half a page long, etc.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#Check the list of stopwords in english language\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Paragraphs are the building blocks of papers.', 'Many students define paragraphs in terms of length.', 'A paragraph is a group of at least five sentences.', 'Paragraph is half a page long, etc.']\n"
     ]
    }
   ],
   "source": [
    "#generate sentences from the paragraph\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words before stemming :  ['Paragraphs', 'are', 'the', 'building', 'blocks', 'of', 'papers', '.']\n",
      "Words after stemming :  ['paragraph', 'build', 'block', 'paper', '.']\n",
      "Words before stemming :  ['Many', 'students', 'define', 'paragraphs', 'in', 'terms', 'of', 'length', '.']\n",
      "Words after stemming :  ['mani', 'student', 'defin', 'paragraph', 'term', 'length', '.']\n",
      "Words before stemming :  ['A', 'paragraph', 'is', 'a', 'group', 'of', 'at', 'least', 'five', 'sentences', '.']\n",
      "Words after stemming :  ['A', 'paragraph', 'group', 'least', 'five', 'sentenc', '.']\n",
      "Words before stemming :  ['Paragraph', 'is', 'half', 'a', 'page', 'long', ',', 'etc', '.']\n",
      "Words after stemming :  ['paragraph', 'half', 'page', 'long', ',', 'etc', '.']\n",
      "Sentences after stemming :  ['paragraph build block paper .', 'mani student defin paragraph term length .', 'A paragraph group least five sentenc .', 'paragraph half page long , etc .']\n"
     ]
    }
   ],
   "source": [
    "#initalise stemmer and stem each word, remove stopwords\n",
    "stemmer = PorterStemmer()\n",
    "stem_sentences = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    print(\"Words before stemming : \", words)\n",
    "    \n",
    "    stem_words = []\n",
    "    for word in words:\n",
    "        if word not in set(stopwords.words('english')):\n",
    "            stem_word = stemmer.stem(word)\n",
    "            stem_words.append(stem_word)\n",
    "    \n",
    "    stem_sentence = ' '.join(stem_words)\n",
    "    stem_sentences.append(stem_sentence)\n",
    "        \n",
    "    print(\"Words after stemming : \", stem_words)\n",
    "    \n",
    "print(\"Sentences after stemming : \", stem_sentences)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes :**\n",
    "\n",
    "* Stemming will lead to words which are not understood by humans. \n",
    "* Counter Intutive to do this when we need the human readble words as input for a problem like NLU, or ChatBot or Text Generation\n",
    "* Lemmatization solves this issue and we pay it through complexity and time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
