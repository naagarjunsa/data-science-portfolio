{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT PREPROCESSING - BAG OF WORDS\n",
    "\n",
    "**Bag of Words** model is a way of representing text; it is a feature extraction exercise to make the representation handy to process. It basically represents count of occurence of words within a corpus. A vocabulary of known words and measure of presence of those words.\n",
    "\n",
    "**Measure of words** can be just binary (if it exists or not), count (how many times it exists), \n",
    "\n",
    "* Clearly the order/structure of the words is discarded\n",
    "* The intuition is that documents are similar if they have similar content. \n",
    "* Further, that from the content alone we can learn something about the meaning of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will be using NLTK to preprocess text\n",
    "# will be using stemming as the task with BOW will not need lot of semantics in the first place\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "paragraph = \"\"\"Jerry's main friends are George Costanza, Cosmo Kramer and his ex-girlfriend Elaine Benes.\\\n",
    "Jerry (though not without exceptions) typically represents the voice of reason amidst George, Elaine, and \\\n",
    "Kramer's antics, and can be seen as the focal point of the foursome's relationship. Jerry is somewhat of \\\n",
    "an eternal optimist, as he rarely runs into major personal problems. Jerry is the only main character on \\\n",
    "the show to maintain the same career throughout the series. Considering his job as a comedian, he is the most \\\n",
    "observational character, usually sarcastically commenting on his friends' quirky habits, almost essentially \\\n",
    "the New York Jew-type character. He seems to have a new girlfriend every week, but the relationships usually \\\n",
    "end for fairly superficial reasons. He is also an almost obsessive compulsive neat freak; he once threw out\\\n",
    "belt because it had touched a urinal, and once commented on finding out his toilet brush had been placed in \\\n",
    "the toilet that, I can replace that.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Jerry's main friends are George Costanza, Cosmo Kramer and his ex-girlfriend Elaine Benes.Jerry (though not without exceptions) typically represents the voice of reason amidst George, Elaine, and Kramer's antics, and can be seen as the focal point of the foursome's relationship.\", 'Jerry is somewhat of an eternal optimist, as he rarely runs into major personal problems.', 'Jerry is the only main character on the show to maintain the same career throughout the series.', \"Considering his job as a comedian, he is the most observational character, usually sarcastically commenting on his friends' quirky habits, almost essentially the New York Jew-type character.\", 'He seems to have a new girlfriend every week, but the relationships usually end for fairly superficial reasons.', 'He is also an almost obsessive compulsive neat freak; he once threw outbelt because it had touched a urinal, and once commented on finding out his toilet brush had been placed in the toilet that, I can replace that.']\n"
     ]
    }
   ],
   "source": [
    "#generate sentences from the paragraph\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words before lemmatization :  ['Jerry', \"'s\", 'main', 'friends', 'are', 'George', 'Costanza', ',', 'Cosmo', 'Kramer', 'and', 'his', 'ex-girlfriend', 'Elaine', 'Benes.Jerry', '(', 'though', 'not', 'without', 'exceptions', ')', 'typically', 'represents', 'the', 'voice', 'of', 'reason', 'amidst', 'George', ',', 'Elaine', ',', 'and', 'Kramer', \"'s\", 'antics', ',', 'and', 'can', 'be', 'seen', 'as', 'the', 'focal', 'point', 'of', 'the', 'foursome', \"'s\", 'relationship', '.']\n",
      "Words after lemmatizaton :  ['Jerry', \"'s\", 'main', 'friend', 'George', 'Costanza', ',', 'Cosmo', 'Kramer', 'ex-girlfriend', 'Elaine', 'Benes.Jerry', '(', 'though', 'without', 'exception', ')', 'typically', 'represents', 'voice', 'reason', 'amidst', 'George', ',', 'Elaine', ',', 'Kramer', \"'s\", 'antic', ',', 'seen', 'focal', 'point', 'foursome', \"'s\", 'relationship', '.']\n",
      "Words before lemmatization :  ['Jerry', 'is', 'somewhat', 'of', 'an', 'eternal', 'optimist', ',', 'as', 'he', 'rarely', 'runs', 'into', 'major', 'personal', 'problems', '.']\n",
      "Words after lemmatizaton :  ['Jerry', 'somewhat', 'eternal', 'optimist', ',', 'rarely', 'run', 'major', 'personal', 'problem', '.']\n",
      "Words before lemmatization :  ['Jerry', 'is', 'the', 'only', 'main', 'character', 'on', 'the', 'show', 'to', 'maintain', 'the', 'same', 'career', 'throughout', 'the', 'series', '.']\n",
      "Words after lemmatizaton :  ['Jerry', 'main', 'character', 'show', 'maintain', 'career', 'throughout', 'series', '.']\n",
      "Words before lemmatization :  ['Considering', 'his', 'job', 'as', 'a', 'comedian', ',', 'he', 'is', 'the', 'most', 'observational', 'character', ',', 'usually', 'sarcastically', 'commenting', 'on', 'his', 'friends', \"'\", 'quirky', 'habits', ',', 'almost', 'essentially', 'the', 'New', 'York', 'Jew-type', 'character', '.']\n",
      "Words after lemmatizaton :  ['Considering', 'job', 'comedian', ',', 'observational', 'character', ',', 'usually', 'sarcastically', 'commenting', 'friend', \"'\", 'quirky', 'habit', ',', 'almost', 'essentially', 'New', 'York', 'Jew-type', 'character', '.']\n",
      "Words before lemmatization :  ['He', 'seems', 'to', 'have', 'a', 'new', 'girlfriend', 'every', 'week', ',', 'but', 'the', 'relationships', 'usually', 'end', 'for', 'fairly', 'superficial', 'reasons', '.']\n",
      "Words after lemmatizaton :  ['He', 'seems', 'new', 'girlfriend', 'every', 'week', ',', 'relationship', 'usually', 'end', 'fairly', 'superficial', 'reason', '.']\n",
      "Words before lemmatization :  ['He', 'is', 'also', 'an', 'almost', 'obsessive', 'compulsive', 'neat', 'freak', ';', 'he', 'once', 'threw', 'outbelt', 'because', 'it', 'had', 'touched', 'a', 'urinal', ',', 'and', 'once', 'commented', 'on', 'finding', 'out', 'his', 'toilet', 'brush', 'had', 'been', 'placed', 'in', 'the', 'toilet', 'that', ',', 'I', 'can', 'replace', 'that', '.']\n",
      "Words after lemmatizaton :  ['He', 'also', 'almost', 'obsessive', 'compulsive', 'neat', 'freak', ';', 'threw', 'outbelt', 'touched', 'urinal', ',', 'commented', 'finding', 'toilet', 'brush', 'placed', 'toilet', ',', 'I', 'replace', '.']\n",
      "Sentences after lemmatizaton :  [\"Jerry 's main friend George Costanza , Cosmo Kramer ex-girlfriend Elaine Benes.Jerry ( though without exception ) typically represents voice reason amidst George , Elaine , Kramer 's antic , seen focal point foursome 's relationship .\", 'Jerry somewhat eternal optimist , rarely run major personal problem .', 'Jerry main character show maintain career throughout series .', \"Considering job comedian , observational character , usually sarcastically commenting friend ' quirky habit , almost essentially New York Jew-type character .\", 'He seems new girlfriend every week , relationship usually end fairly superficial reason .', 'He also almost obsessive compulsive neat freak ; threw outbelt touched urinal , commented finding toilet brush placed toilet , I replace .']\n"
     ]
    }
   ],
   "source": [
    "#initalise stemmer and stem each word, remove stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_sentences = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    print(\"Words before lemmatization : \", words)\n",
    "    \n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        if word not in set(stopwords.words('english')):\n",
    "            lemma = lemmatizer.lemmatize(word)\n",
    "            lemmas.append(lemma)\n",
    "    \n",
    "    lemmatized_sentence = ' '.join(lemmas)\n",
    "    lemmatized_sentences.append(lemmatized_sentence)\n",
    "        \n",
    "    print(\"Words after lemmatizaton : \", lemmas)\n",
    "    \n",
    "print(\"Sentences after lemmatizaton : \", lemmatized_sentences) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word : almost count : 0 \n",
      "word : also count : 0 \n",
      "word : amidst count : 1 \n",
      "word : antic count : 1 \n",
      "word : benes count : 1 \n",
      "word : brush count : 0 \n",
      "word : career count : 0 \n",
      "word : character count : 0 \n",
      "word : comedian count : 0 \n",
      "word : commented count : 0 \n",
      "word : commenting count : 0 \n",
      "word : compulsive count : 0 \n",
      "word : considering count : 0 \n",
      "word : cosmo count : 1 \n",
      "word : costanza count : 1 \n",
      "word : elaine count : 2 \n",
      "word : end count : 0 \n",
      "word : essentially count : 0 \n",
      "word : eternal count : 0 \n",
      "word : every count : 0 \n",
      "word : ex count : 1 \n",
      "word : exception count : 1 \n",
      "word : fairly count : 0 \n",
      "word : finding count : 0 \n",
      "word : focal count : 1 \n",
      "word : foursome count : 1 \n",
      "word : freak count : 0 \n",
      "word : friend count : 1 \n",
      "word : george count : 2 \n",
      "word : girlfriend count : 1 \n",
      "word : habit count : 0 \n",
      "word : he count : 0 \n",
      "word : jerry count : 2 \n",
      "word : jew count : 0 \n",
      "word : job count : 0 \n",
      "word : kramer count : 2 \n",
      "word : main count : 1 \n",
      "word : maintain count : 0 \n",
      "word : major count : 0 \n",
      "word : neat count : 0 \n",
      "word : new count : 0 \n",
      "word : observational count : 0 \n",
      "word : obsessive count : 0 \n",
      "word : optimist count : 0 \n",
      "word : outbelt count : 0 \n",
      "word : personal count : 0 \n",
      "word : placed count : 0 \n",
      "word : point count : 1 \n",
      "word : problem count : 0 \n",
      "word : quirky count : 0 \n",
      "word : rarely count : 0 \n",
      "word : reason count : 1 \n",
      "word : relationship count : 1 \n",
      "word : replace count : 0 \n",
      "word : represents count : 1 \n",
      "word : run count : 0 \n",
      "word : sarcastically count : 0 \n",
      "word : seems count : 0 \n",
      "word : seen count : 1 \n",
      "word : series count : 0 \n",
      "word : show count : 0 \n",
      "word : somewhat count : 0 \n",
      "word : superficial count : 0 \n",
      "word : though count : 1 \n",
      "word : threw count : 0 \n",
      "word : throughout count : 0 \n",
      "word : toilet count : 0 \n",
      "word : touched count : 0 \n",
      "word : type count : 0 \n",
      "word : typically count : 1 \n",
      "word : urinal count : 0 \n",
      "word : usually count : 0 \n",
      "word : voice count : 1 \n",
      "word : week count : 0 \n",
      "word : without count : 1 \n",
      "word : york count : 0 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vector = count_vectorizer.fit_transform(lemmatized_sentences)\n",
    "\n",
    "for count, word in zip(count_vector.toarray().tolist()[0], count_vectorizer.get_feature_names()) :\n",
    "    print(\"word : {} count : {} \".format(word, count))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** :\n",
    "\n",
    "* The **measure of words** can be more intelligent by giving weight to words which are more important (rarity like nouns)\n",
    "* TF-IDF will be discussed later which makes that measure intelligent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
