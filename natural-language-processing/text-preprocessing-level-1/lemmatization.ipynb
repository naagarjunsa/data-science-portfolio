{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT PREPROCESSING - LEMMATIZATION\n",
    "\n",
    "**Lemmatization** is text preprocessing techinique where the tokens generated from the corpus are reduced to their dictionary form a.k.a lemma. The lemmas are meaningful words, canonical forms of the words. This makes processing more complex and slower than stemming. This is preferred when the problem needs meaningful words as input (text generation) than just a symbolic represtation (text classification - stemming is enough for this).\n",
    "\n",
    "Lemmatization involves Part of Speech Tagging for better word to lemma conversion.\n",
    "Harder to make a lemmatizer for new language (than a stemmer)\n",
    "NLTK Lemmatizer is based on **WordNet database** - like a thesaurus.\n",
    "\n",
    "**StopWord Removal** is still a necessity to reduce less semantically important words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#will be using NLTK to demonstrate lemmatizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "paragraph = \"\"\"Paragraphs are the building blocks of papers. Many students define paragraphs \\\n",
    "in terms of length. A paragraph is a group of at least five sentences. Paragraph \\\n",
    "is half a page long, etc.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#Check the list of stopwords in english language\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Paragraphs are the building blocks of papers.', 'Many students define paragraphs in terms of length.', 'A paragraph is a group of at least five sentences.', 'Paragraph is half a page long, etc.']\n"
     ]
    }
   ],
   "source": [
    "#generate sentences from the paragraph\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words before lemmatization :  ['Paragraphs', 'are', 'the', 'building', 'blocks', 'of', 'papers', '.']\n",
      "Words after lemmatizaton :  ['Paragraphs', 'building', 'block', 'paper', '.']\n",
      "Words before lemmatization :  ['Many', 'students', 'define', 'paragraphs', 'in', 'terms', 'of', 'length', '.']\n",
      "Words after lemmatizaton :  ['Many', 'student', 'define', 'paragraph', 'term', 'length', '.']\n",
      "Words before lemmatization :  ['A', 'paragraph', 'is', 'a', 'group', 'of', 'at', 'least', 'five', 'sentences', '.']\n",
      "Words after lemmatizaton :  ['A', 'paragraph', 'group', 'least', 'five', 'sentence', '.']\n",
      "Words before lemmatization :  ['Paragraph', 'is', 'half', 'a', 'page', 'long', ',', 'etc', '.']\n",
      "Words after lemmatizaton :  ['Paragraph', 'half', 'page', 'long', ',', 'etc', '.']\n",
      "Sentences after lemmatizaton :  ['Paragraphs building block paper .', 'Many student define paragraph term length .', 'A paragraph group least five sentence .', 'Paragraph half page long , etc .']\n"
     ]
    }
   ],
   "source": [
    "#initalise stemmer and stem each word, remove stopwords\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_sentences = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    print(\"Words before lemmatization : \", words)\n",
    "    \n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        if word not in set(stopwords.words('english')):\n",
    "            lemma = lemmatizer.lemmatize(word)\n",
    "            lemmas.append(lemma)\n",
    "    \n",
    "    lemmatized_sentence = ' '.join(lemmas)\n",
    "    lemmatized_sentences.append(lemmatized_sentence)\n",
    "        \n",
    "    print(\"Words after lemmatizaton : \", lemmas)\n",
    "    \n",
    "print(\"Sentences after lemmatizaton : \", lemmatized_sentences)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes :**\n",
    "\n",
    "* Lemmatization solves the issues of Stemming but we pay it through complexity and time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
