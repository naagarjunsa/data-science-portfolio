{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT PREPROCESSING - TF- IDF\n",
    "\n",
    "**TF-IDF stands for Term Frequency - Inverse Document Frequency**\n",
    "\n",
    "This approach is more intelligent than just COUNT or BINARY ( as in Bag Of Words)\n",
    "\n",
    "**Term Frequency** is a measure of term wrt the sentence. i.e (#term in sentence/ # total terms in sentence).\n",
    "Reward words having high occurrence in a document - Frequent\n",
    "\n",
    "**Inverse Document Frequency** is a measure of each term wrt to document.i.e. log of(# of sentences/ #sentences with the term)\n",
    "Penalize words appearing many times in a document collection. Too general words should not have have high weight eg. \"or\" \"not\" \"is\" \"the\" - Reward Rarity\n",
    "\n",
    "Both terms are multiplied to give us the tf-idf value of each term wrt the sentence.\n",
    "\n",
    "Hence word rare in a document collection but frequent in a particular document get high weight.\n",
    "\n",
    "In other word, combining TF and IDF together => assign high weight to discriminative words in a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Natural Language Tool Kit (NLTK) is a NLP package for text data processing\n",
    "import nltk\n",
    "\n",
    "paragraph = \"\"\"Paragraphs are the building blocks of papers. Many students define paragraphs \\\n",
    "in terms of length. A paragraph is a group of at least five sentences. Paragraph \\\n",
    "is half a page long, etc.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Paragraphs are the building blocks of papers.',\n",
       " 'Many students define paragraphs in terms of length.',\n",
       " 'A paragraph is a group of at least five sentences.',\n",
       " 'Paragraph is half a page long, etc.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words before lemmatization :  ['Paragraphs', 'are', 'the', 'building', 'blocks', 'of', 'papers']\n",
      "Words after lemmatizaton :  ['paragraphs', 'building', 'block', 'paper']\n",
      "Words before lemmatization :  ['Many', 'students', 'define', 'paragraphs', 'in', 'terms', 'of', 'length']\n",
      "Words after lemmatizaton :  ['many', 'student', 'define', 'paragraph', 'term', 'length']\n",
      "Words before lemmatization :  ['A', 'paragraph', 'is', 'a', 'group', 'of', 'at', 'least', 'five', 'sentences']\n",
      "Words after lemmatizaton :  ['a', 'paragraph', 'group', 'least', 'five', 'sentence']\n",
      "Words before lemmatization :  ['Paragraph', 'is', 'half', 'a', 'page', 'long', 'etc']\n",
      "Words after lemmatizaton :  ['paragraph', 'half', 'page', 'long', 'etc']\n",
      "Sentences after lemmatizaton :  ['paragraphs building block paper', 'many student define paragraph term length', 'a paragraph group least five sentence', 'paragraph half page long etc']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_sentences = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    print(\"Words before lemmatization : \", words)\n",
    "    \n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        if word not in set(stopwords.words('english')):\n",
    "            lemma = lemmatizer.lemmatize(word)\n",
    "            lemma = lemma.lower()\n",
    "            lemmas.append(lemma)\n",
    "    \n",
    "    lemmatized_sentence = ' '.join(lemmas)\n",
    "    lemmatized_sentences.append(lemmatized_sentence)\n",
    "        \n",
    "    print(\"Words after lemmatizaton : \", lemmas)\n",
    "    \n",
    "print(\"Sentences after lemmatizaton : \", lemmatized_sentences)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word : block count : 0.5 \n",
      "word : building count : 0.5 \n",
      "word : define count : 0.0 \n",
      "word : etc count : 0.0 \n",
      "word : five count : 0.0 \n",
      "word : group count : 0.0 \n",
      "word : half count : 0.0 \n",
      "word : least count : 0.0 \n",
      "word : length count : 0.0 \n",
      "word : long count : 0.0 \n",
      "word : many count : 0.0 \n",
      "word : page count : 0.0 \n",
      "word : paper count : 0.5 \n",
      "word : paragraph count : 0.0 \n",
      "word : paragraphs count : 0.5 \n",
      "word : sentence count : 0.0 \n",
      "word : student count : 0.0 \n",
      "word : term count : 0.0 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "tf_idf_vector = tf_idf_vectorizer.fit_transform(lemmatized_sentences)\n",
    "\n",
    "for count, word in zip(tf_idf_vector.toarray().tolist()[0], tf_idf_vectorizer.get_feature_names()) :\n",
    "    print(\"word : {} count : {} \".format(word, count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
