{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT PREPROCESSING - TOKENIZATION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text preprocessing is a very integral part of any Natural Language Processing. Although we currently have immensely powerful methods of text preprocessing, an intuition about the traditional methods will be a solid foundation to understanding the things to come. \n",
    "\n",
    "The inital text preprocessing concept we will be discussing is **Tokenization**. It is bascially breaking down text into smaller pieces - a.k.a tokens. This helps in developing a NLP model or understanding the semantics of the textual data. \n",
    "\n",
    "_Here is an example of tokenization_\n",
    "\n",
    "| Before Tokenization | After Tokenization |\n",
    "| --- | --- | \n",
    "| \"This is a test sentence\" | 'This' 'is' 'a' 'test' 'sentence' |\n",
    "\n",
    "\n",
    "We will be using NLTK and Keras Libraries to illustrate the concepts. Will be adding Gensim, Spacy, TextBlob examples later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Natural Language Tool Kit (NLTK) is a NLP package for text data processing\n",
    "import nltk\n",
    "\n",
    "paragraph = \"\"\"Paragraphs are the building blocks of papers. Many students define paragraphs \\\n",
    "in terms of length. A paragraph is a group of at least five sentences. Paragraph \\\n",
    "is half a page long, etc.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Paragraphs are the building blocks of papers.', 'Many students define paragraphs in terms of length.', 'A paragraph is a group of at least five sentences.', 'Paragraph is half a page long, etc.']\n"
     ]
    }
   ],
   "source": [
    "#nltk.sent_tokenize() lists the sentences in raw data\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Paragraphs', 'are', 'the', 'building', 'blocks', 'of', 'papers', '.', 'Many', 'students', 'define', 'paragraphs', 'in', 'terms', 'of', 'length', '.', 'A', 'paragraph', 'is', 'a', 'group', 'of', 'at', 'least', 'five', 'sentences', '.', 'Paragraph', 'is', 'half', 'a', 'page', 'long', ',', 'etc', '.']\n"
     ]
    }
   ],
   "source": [
    "#nltk.word_tokenize() lists the sentences in raw data\n",
    "words = nltk.word_tokenize(paragraph)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Martinelli', 'to', 'start', 'today', '<3', '.', 'Hope', 'he', 'recreates', 'magic', 'at', '#StamfordBridge', '#ARSCHE', '#COYG']\n",
      "['Martinelli', 'to', 'start', 'today', '<', '3', '.', 'Hope', 'he', 'recreates', 'magic', 'at', '#', 'StamfordBridge', '#', 'ARSCHE', '#', 'COYG']\n"
     ]
    }
   ],
   "source": [
    "#nltk.tweet_tokenize() for tokenizing tweets - handles emojis and hashtags\n",
    "\n",
    "tweet = \" Martinelli to start today <3. Hope he recreates magic at #StamfordBridge #ARSCHE #COYG\"\n",
    "\n",
    "tweet_tokenizer = nltk.TweetTokenizer()\n",
    "tweet_tokens = tweet_tokenizer.tokenize(tweet)\n",
    "print(tweet_tokens)\n",
    "\n",
    "normal_tokens = nltk.word_tokenize(tweet)\n",
    "print(normal_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'earned', '$400', 'as', 'winnings', 'of', 'Microsoft', 'Hackathon', '.']\n",
      "['I', 'earned', '$', '400', 'as', 'winnings', 'of', 'Microsoft', 'Hackathon', '.']\n"
     ]
    }
   ],
   "source": [
    "#nltk.RegexpTokenizer() for splits the text into tokens based on regex\n",
    "\n",
    "sample_sentence = \"I earned $400 as winnings of Microsoft Hackathon.\"\n",
    "regex_tokenizer = nltk.RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
    "regex_tokens = regex_tokenizer.tokenize(sample_sentence)\n",
    "print(regex_tokens)\n",
    "\n",
    "normal_tokens = nltk.word_tokenize(sample_sentence)\n",
    "print(normal_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization can also be done by using keras - a deep learning API written in Python\n",
    "import keras\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "words = text_to_word_sequence(paragraph)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenges with Tokenization :**\n",
    "\n",
    "* Boundary of words - Difficult to find the end of words for no-space languages like Chinese etc\n",
    "* Lot of Symbols and other noise in real life text data\n",
    "* Short Forms in the Language - I am (I'm) etc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
