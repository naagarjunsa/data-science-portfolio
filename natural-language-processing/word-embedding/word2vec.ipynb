{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORD2VEC\n",
    "\n",
    "The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Each vector has some semantic meaning to it. Created with shallow 2 layered NN that reconstruct the context of words. Helps in developing context for each word using embeddings. \n",
    "\n",
    "Developed in either of the two model archs:\n",
    "\n",
    "1. CBOW - Continuous Bag of Words - model predicts current word from surrounding words. ( No order of context, faster, distant also better) \n",
    "2. Skip Gram - model predicts surrounding windows from current word. (context is order, slower, closer ones more important)\n",
    "\n",
    "Hyper Parameters involved:\n",
    "\n",
    "1. Training algorithm - hierarchical softmax and/or negative sampling. hierarchical softmax works better for infrequent words while negative sampling works better for frequent words and better with low dimensional vectors.\n",
    "\n",
    "2. Sub Sampling - High-frequency words often provide little information. Words with a frequency above a certain threshold may be subsampled to increase training speed\n",
    "\n",
    "3. Dimensionality - After a point of increased embedding size, no point. Usually 100 to 1000 is the size.\n",
    "\n",
    "4. Context Window - number of Surrounding words - 10 for skip gram, 5 for CBOW \n",
    "\n",
    "Exercise is to train own word2vec model and play with pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"WORD2VEC\n",
    "The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Each vector has some semantic meaning to it. Created with shallow 2 layered NN that reconstruct the context of words. Helps in developing context for each word using embeddings.\n",
    "\n",
    "Developed in either of the two model archs:\n",
    "\n",
    "CBOW - Continuous Bag of Words - model predicts current word from surrounding words. ( No order of context, faster, distant also better)\n",
    "Skip Gram - model predicts surrounding windows from current word. (context is order, slower, closer ones more important)\n",
    "Hyper Parameters involved:\n",
    "\n",
    "Training algorithm - hierarchical softmax and/or negative sampling. hierarchical softmax works better for infrequent words while negative sampling works better for frequent words and better with low dimensional vectors.\n",
    "\n",
    "Sub Sampling - High-frequency words often provide little information. Words with a frequency above a certain threshold may be subsampled to increase training speed\n",
    "\n",
    "Dimensionality - After a point of increased embedding size, no point. Usually 100 to 1000 is the size.\n",
    "\n",
    "Context Window - number of Surrounding words - 10 for skip gram, 5 for CBOW\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence before processing :  WORD2VEC\n",
      "The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text.\n",
      "\n",
      "Sentence after processing :  ['word2vec', 'word2vec', 'algorithm', 'uses', 'neural', 'network', 'model', 'learn', 'word', 'associations', 'large', 'corpus', 'text']\n",
      "\n",
      "Sentence before processing :  Each vector has some semantic meaning to it.\n",
      "\n",
      "Sentence after processing :  ['vector', 'semantic', 'meaning']\n",
      "\n",
      "Sentence before processing :  Created with shallow 2 layered NN that reconstruct the context of words.\n",
      "\n",
      "Sentence after processing :  ['created', 'shallow', '2', 'layered', 'nn', 'reconstruct', 'context', 'words']\n",
      "\n",
      "Sentence before processing :  Helps in developing context for each word using embeddings.\n",
      "\n",
      "Sentence after processing :  ['helps', 'developing', 'context', 'word', 'using', 'embeddings']\n",
      "\n",
      "Sentence before processing :  Developed in either of the two model archs:\n",
      "\n",
      "CBOW - Continuous Bag of Words - model predicts current word from surrounding words.\n",
      "\n",
      "Sentence after processing :  ['developed', 'either', 'two', 'model', 'archs', 'cbow', 'continuous', 'bag', 'words', 'model', 'predicts', 'current', 'word', 'surrounding', 'words']\n",
      "\n",
      "Sentence before processing :  ( No order of context, faster, distant also better)\n",
      "Skip Gram - model predicts surrounding windows from current word.\n",
      "\n",
      "Sentence after processing :  ['order', 'context', 'faster', 'distant', 'also', 'better', 'skip', 'gram', 'model', 'predicts', 'surrounding', 'windows', 'current', 'word']\n",
      "\n",
      "Sentence before processing :  (context is order, slower, closer ones more important)\n",
      "Hyper Parameters involved:\n",
      "\n",
      "Training algorithm - hierarchical softmax and/or negative sampling.\n",
      "\n",
      "Sentence after processing :  ['context', 'order', 'slower', 'closer', 'ones', 'important', 'hyper', 'parameters', 'involved', 'training', 'algorithm', 'hierarchical', 'softmax', 'negative', 'sampling']\n",
      "\n",
      "Sentence before processing :  hierarchical softmax works better for infrequent words while negative sampling works better for frequent words and better with low dimensional vectors.\n",
      "\n",
      "Sentence after processing :  ['hierarchical', 'softmax', 'works', 'better', 'infrequent', 'words', 'negative', 'sampling', 'works', 'better', 'frequent', 'words', 'better', 'low', 'dimensional', 'vectors']\n",
      "\n",
      "Sentence before processing :  Sub Sampling - High-frequency words often provide little information.\n",
      "\n",
      "Sentence after processing :  ['sub', 'sampling', 'high', 'frequency', 'words', 'often', 'provide', 'little', 'information']\n",
      "\n",
      "Sentence before processing :  Words with a frequency above a certain threshold may be subsampled to increase training speed\n",
      "\n",
      "Dimensionality - After a point of increased embedding size, no point.\n",
      "\n",
      "Sentence after processing :  ['words', 'frequency', 'certain', 'threshold', 'may', 'subsampled', 'increase', 'training', 'speed', 'dimensionality', 'point', 'increased', 'embedding', 'size', 'point']\n",
      "\n",
      "Sentence before processing :  Usually 100 to 1000 is the size.\n",
      "\n",
      "Sentence after processing :  ['usually', '100', '1000', 'size']\n",
      "\n",
      "Sentence before processing :  Context Window - number of Surrounding words - 10 for skip gram, 5 for CBOW\n",
      "\n",
      "Sentence after processing :  ['context', 'window', 'number', 'surrounding', 'words', '10', 'skip', 'gram', '5', 'cbow']\n"
     ]
    }
   ],
   "source": [
    "#preprocess the data using regex\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "processed_sentences = []\n",
    "for sentence in sentences:\n",
    "    print(\"\\nSentence before processing : \", sentence)\n",
    "    sentence = re.sub('[^a-zA-Z0-9]', ' ',sentence)\n",
    "    sentence = re.sub('\\s+', ' ', sentence)\n",
    "    sentence = sentence.lower()\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    processed_sentence = [word for word in words if word not in stopwords.words('english')]\n",
    "    \n",
    "    processed_sentences.append(processed_sentence)\n",
    "        \n",
    "    print(\"\\nSentence after processing : \", processed_sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec  :  Vocab(count:2, index:7, sample_int:1361428237)\n",
      "algorithm  :  Vocab(count:2, index:8, sample_int:1361428237)\n",
      "uses  :  Vocab(count:1, index:23, sample_int:2086370027)\n",
      "neural  :  Vocab(count:1, index:24, sample_int:2086370027)\n",
      "network  :  Vocab(count:1, index:25, sample_int:2086370027)\n",
      "model  :  Vocab(count:4, index:2, sample_int:905746060)\n",
      "learn  :  Vocab(count:1, index:26, sample_int:2086370027)\n",
      "word  :  Vocab(count:4, index:3, sample_int:905746060)\n",
      "associations  :  Vocab(count:1, index:27, sample_int:2086370027)\n",
      "large  :  Vocab(count:1, index:28, sample_int:2086370027)\n",
      "corpus  :  Vocab(count:1, index:29, sample_int:2086370027)\n",
      "text  :  Vocab(count:1, index:30, sample_int:2086370027)\n",
      "vector  :  Vocab(count:1, index:31, sample_int:2086370027)\n",
      "semantic  :  Vocab(count:1, index:32, sample_int:2086370027)\n",
      "meaning  :  Vocab(count:1, index:33, sample_int:2086370027)\n",
      "created  :  Vocab(count:1, index:34, sample_int:2086370027)\n",
      "shallow  :  Vocab(count:1, index:35, sample_int:2086370027)\n",
      "2  :  Vocab(count:1, index:36, sample_int:2086370027)\n",
      "layered  :  Vocab(count:1, index:37, sample_int:2086370027)\n",
      "nn  :  Vocab(count:1, index:38, sample_int:2086370027)\n",
      "reconstruct  :  Vocab(count:1, index:39, sample_int:2086370027)\n",
      "context  :  Vocab(count:5, index:1, sample_int:797145930)\n",
      "words  :  Vocab(count:8, index:0, sample_int:611994642)\n",
      "helps  :  Vocab(count:1, index:40, sample_int:2086370027)\n",
      "developing  :  Vocab(count:1, index:41, sample_int:2086370027)\n",
      "using  :  Vocab(count:1, index:42, sample_int:2086370027)\n",
      "embeddings  :  Vocab(count:1, index:43, sample_int:2086370027)\n",
      "developed  :  Vocab(count:1, index:44, sample_int:2086370027)\n",
      "either  :  Vocab(count:1, index:45, sample_int:2086370027)\n",
      "two  :  Vocab(count:1, index:46, sample_int:2086370027)\n",
      "archs  :  Vocab(count:1, index:47, sample_int:2086370027)\n",
      "cbow  :  Vocab(count:2, index:9, sample_int:1361428237)\n",
      "continuous  :  Vocab(count:1, index:48, sample_int:2086370027)\n",
      "bag  :  Vocab(count:1, index:49, sample_int:2086370027)\n",
      "predicts  :  Vocab(count:2, index:10, sample_int:1361428237)\n",
      "current  :  Vocab(count:2, index:11, sample_int:1361428237)\n",
      "surrounding  :  Vocab(count:3, index:5, sample_int:1070416568)\n",
      "order  :  Vocab(count:2, index:12, sample_int:1361428237)\n",
      "faster  :  Vocab(count:1, index:50, sample_int:2086370027)\n",
      "distant  :  Vocab(count:1, index:51, sample_int:2086370027)\n",
      "also  :  Vocab(count:1, index:52, sample_int:2086370027)\n",
      "better  :  Vocab(count:4, index:4, sample_int:905746060)\n",
      "skip  :  Vocab(count:2, index:13, sample_int:1361428237)\n",
      "gram  :  Vocab(count:2, index:14, sample_int:1361428237)\n",
      "windows  :  Vocab(count:1, index:53, sample_int:2086370027)\n",
      "slower  :  Vocab(count:1, index:54, sample_int:2086370027)\n",
      "closer  :  Vocab(count:1, index:55, sample_int:2086370027)\n",
      "ones  :  Vocab(count:1, index:56, sample_int:2086370027)\n",
      "important  :  Vocab(count:1, index:57, sample_int:2086370027)\n",
      "hyper  :  Vocab(count:1, index:58, sample_int:2086370027)\n",
      "parameters  :  Vocab(count:1, index:59, sample_int:2086370027)\n",
      "involved  :  Vocab(count:1, index:60, sample_int:2086370027)\n",
      "training  :  Vocab(count:2, index:15, sample_int:1361428237)\n",
      "hierarchical  :  Vocab(count:2, index:16, sample_int:1361428237)\n",
      "softmax  :  Vocab(count:2, index:17, sample_int:1361428237)\n",
      "negative  :  Vocab(count:2, index:18, sample_int:1361428237)\n",
      "sampling  :  Vocab(count:3, index:6, sample_int:1070416568)\n",
      "works  :  Vocab(count:2, index:19, sample_int:1361428237)\n",
      "infrequent  :  Vocab(count:1, index:61, sample_int:2086370027)\n",
      "frequent  :  Vocab(count:1, index:62, sample_int:2086370027)\n",
      "low  :  Vocab(count:1, index:63, sample_int:2086370027)\n",
      "dimensional  :  Vocab(count:1, index:64, sample_int:2086370027)\n",
      "vectors  :  Vocab(count:1, index:65, sample_int:2086370027)\n",
      "sub  :  Vocab(count:1, index:66, sample_int:2086370027)\n",
      "high  :  Vocab(count:1, index:67, sample_int:2086370027)\n",
      "frequency  :  Vocab(count:2, index:20, sample_int:1361428237)\n",
      "often  :  Vocab(count:1, index:68, sample_int:2086370027)\n",
      "provide  :  Vocab(count:1, index:69, sample_int:2086370027)\n",
      "little  :  Vocab(count:1, index:70, sample_int:2086370027)\n",
      "information  :  Vocab(count:1, index:71, sample_int:2086370027)\n",
      "certain  :  Vocab(count:1, index:72, sample_int:2086370027)\n",
      "threshold  :  Vocab(count:1, index:73, sample_int:2086370027)\n",
      "may  :  Vocab(count:1, index:74, sample_int:2086370027)\n",
      "subsampled  :  Vocab(count:1, index:75, sample_int:2086370027)\n",
      "increase  :  Vocab(count:1, index:76, sample_int:2086370027)\n",
      "speed  :  Vocab(count:1, index:77, sample_int:2086370027)\n",
      "dimensionality  :  Vocab(count:1, index:78, sample_int:2086370027)\n",
      "point  :  Vocab(count:2, index:21, sample_int:1361428237)\n",
      "increased  :  Vocab(count:1, index:79, sample_int:2086370027)\n",
      "embedding  :  Vocab(count:1, index:80, sample_int:2086370027)\n",
      "size  :  Vocab(count:2, index:22, sample_int:1361428237)\n",
      "usually  :  Vocab(count:1, index:81, sample_int:2086370027)\n",
      "100  :  Vocab(count:1, index:82, sample_int:2086370027)\n",
      "1000  :  Vocab(count:1, index:83, sample_int:2086370027)\n",
      "window  :  Vocab(count:1, index:84, sample_int:2086370027)\n",
      "number  :  Vocab(count:1, index:85, sample_int:2086370027)\n",
      "10  :  Vocab(count:1, index:86, sample_int:2086370027)\n",
      "5  :  Vocab(count:1, index:87, sample_int:2086370027)\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(processed_sentences, min_count = 1)\n",
    "\n",
    "vocab = model.wv.vocab\n",
    "for key, value in vocab.items():\n",
    "    print(key, \" : \", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('5', 0.26387906074523926),\n",
       " ('developing', 0.2299186885356903),\n",
       " ('important', 0.19774191081523895),\n",
       " ('size', 0.19406504929065704),\n",
       " ('dimensionality', 0.18159811198711395),\n",
       " ('word2vec', 0.181463822722435),\n",
       " ('closer', 0.16070972383022308),\n",
       " ('network', 0.1579560488462448),\n",
       " ('meaning', 0.1513058990240097),\n",
       " ('using', 0.14122387766838074)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = model.wv['skip']\n",
    "\n",
    "similar = model.wv.most_similar('skip')\n",
    "similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "#pretrained model from gensim repository ( lsiting all avaialable models)\n",
    "import gensim.downloader\n",
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('encyclopedia', 0.5071935653686523),\n",
       " ('wikimedia', 0.5039559602737427),\n",
       " ('wiki', 0.49234116077423096),\n",
       " ('facebook', 0.46857360005378723),\n",
       " ('blog', 0.4539109170436859),\n",
       " ('conservapedia', 0.4523037075996399),\n",
       " ('youtube', 0.45151132345199585),\n",
       " ('britannica', 0.44909369945526123),\n",
       " ('websites', 0.44877538084983826),\n",
       " ('blogs', 0.4350595772266388)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki = gensim.downloader.load('glove-wiki-gigaword-300')\n",
    "\n",
    "glove_wiki.most_similar('wikipedia')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
